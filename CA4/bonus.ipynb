{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2032b74f",
   "metadata": {},
   "source": [
    "**<span style=\"color: gray;\">In The Name of God</span>**\n",
    "\n",
    "---\n",
    "\n",
    "# **Bonus**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb8b6d7",
   "metadata": {},
   "source": [
    "# **<span style=\"color: #99004C;\">Task#1 Bonus</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84a76c",
   "metadata": {},
   "source": [
    "##  When Does an MLP Become Equivalent to Logistic Regression?\n",
    "\n",
    "An **MLP (Multi-Layer Perceptron)** becomes equivalent to **logistic regression** under the following condition:\n",
    "\n",
    "1. **The network has only one linear layer** (**no hidden layers** )\n",
    "2. **Activation function is either:**\n",
    "   - sigmoid\n",
    "   - softmax\n",
    "3. **No Dropout or BatchNorm or complex components**\n",
    "   - Dropout adds random noise to the network (makes the model stochastic) and Logistic regression is a deterministic model.\n",
    "   - BatchNorm introduces a non-linear, dynamic transformation to the data.Logistic regression just applies a fixed linear transformation, no learned normalization0.Also, BatchNorm depends on the other data points in the batch — logistic regression doesn't.\n",
    "\n",
    "4. **No Non-linearities (ReLU, Tanh, etc.)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa84563c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **<span style=\"color: #99004C;\">Task#2 Bonus</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6c073",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Parameters in the Convolutional Layer**\n",
    "\n",
    "#### **Given:**\n",
    "- Input dimensions: `28 × 28 × 3` (height × width × channels)  \n",
    "- Filters: `3` filters, each of size `2 × 2 × 3`  \n",
    "- Padding: `\"valid\"` (no padding,meaning the output shrink by one dimention from each side after each fitler)  \n",
    "\n",
    "#### **Step 1: Parameters per Filter**\n",
    "Each filter has:\n",
    "- **Weights**:  \n",
    "  `kernel_height × kernel_width × input_channels = 2 × 2 × 3 = 12` parameters  \n",
    "- **Bias**:  \n",
    "  `1` parameter (per filter)  \n",
    "\n",
    "**Total per filter** = `12 (weights) + 1 (bias) = 13` parameters.  \n",
    "\n",
    "#### **Step 2: Total Parameters for All Filters** \n",
    "- **Total parameters** = `3 filters × 13 parameters = 39`.  \n",
    "\n",
    "### **Part B: Equivalent Fully Connected Layer**\n",
    "\n",
    "#### **Objective:**  \n",
    "Replicate the convolutional operation using a fully connected (FC) layer.  \n",
    "\n",
    "#### **Step 1: Output Dimensions of Conv Layer**  \n",
    "- Input: `28 × 28 × 3 = 2352` pixels (flattened).  \n",
    "- With `valid` padding and `2×2` kernels:  \n",
    "  Output spatial dimensions = `27 × 27`.  \n",
    "- For `3` filters:  \n",
    "  Total outputs = `3 × 27 × 27 = 2187`.  \n",
    "\n",
    "#### **Step 2: FC Layer Architecture**  \n",
    "- The FC layer must map `2352` inputs → `2187` outputs.  \n",
    "- **Weight matrix**:  \n",
    "  Shape = `(2187, 2352)` → `2187 × 2352 = 5,145,624` weights.  \n",
    "- **Bias terms**:  \n",
    "  `2187` (one per output neuron).  \n",
    "\n",
    "#### **Total FC Parameters:**  \n",
    "`5,145,624 (weights) + 2,187 (biases) = 5,147,811`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76db59",
   "metadata": {},
   "source": [
    "# **<span style=\"color: #99004C;\">Task#3 Bonus</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d4ebf",
   "metadata": {},
   "source": [
    "### **Explain mathematically the vanishing gradient problem in Recurrent Neural Networks (RNNs). Then, analyze how changing the lookback window size impacts the severity of this phenomenon**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d42cb1",
   "metadata": {},
   "source": [
    "**1. Mathematical Explanation**  \n",
    "The gradient in RNNs involves multiplying Jacobians over time:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = \\sum_{k=1}^{T} \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial h_T}}_{\\text{Loss gradient}} \\underbrace{\\prod_{t=k}^{T-1} \\frac{\\partial h_{t+1}}{\\partial h_t}}_{\\text{Jacobian chain}} \\frac{\\partial h_k}{\\partial W}\n",
    "$$  \n",
    "\n",
    "- **Problem:** If $ \\left\\| \\frac{\\partial h_{t+1}}{\\partial h_t} \\right\\| < 1 $ repeated multiplication shrinks gradients exponentially.  \n",
    "- **Result:** Early time steps receive near-zero updates (**vanishing gradients**).  \n",
    "\n",
    "\n",
    "**2. Impact of Lookback Window Size**  \n",
    "| Lookback Window ( T ) | Gradient Behavior | Model Performance |  \n",
    "|--------------------------|-------------------|-------------------|  \n",
    "| **Short** (e.g., \\( T=5 \\))  | Gradients stable | Captures only short-term patterns |  \n",
    "| **Long** (e.g., \\( T=50 \\)) | Gradients vanish exponentially | Fails to learn long-term dependencies |  \n",
    "\n",
    "**Example:** If each Jacobian $ \\approx 0.9 $, after T=50, $ 0.9^{50} \\approx 0.005 $.  \n",
    "\n",
    "\n",
    "**3. Solutions**  \n",
    "1. **LSTM/GRU:** Gated architectures mitigate vanishing gradients.  \n",
    "2. **Skip Connections:** Residual paths (e.g., Transformers) bypass Jacobian chain.  \n",
    "3. **Gradient Clipping:** Prevents exploding gradients.  \n",
    "\n",
    "**Conclusion:** Longer lookback worsens vanishing gradients; use specialized architectures for long sequences.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
