{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "603fb9ae",
   "metadata": {},
   "source": [
    "### 3 - Explain mathematically the vanishing gradient problem in Recurrent Neural Networks (RNNs). Then, analyze how changing the lookback window size impacts the severity of this phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84692804",
   "metadata": {},
   "source": [
    "**1. Mathematical Explanation**  \n",
    "The gradient in RNNs involves multiplying Jacobians over time:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = \\sum_{k=1}^{T} \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial h_T}}_{\\text{Loss gradient}} \\underbrace{\\prod_{t=k}^{T-1} \\frac{\\partial h_{t+1}}{\\partial h_t}}_{\\text{Jacobian chain}} \\frac{\\partial h_k}{\\partial W}\n",
    "$$  \n",
    "\n",
    "- **Problem:** If $ \\left\\| \\frac{\\partial h_{t+1}}{\\partial h_t} \\right\\| < 1 $ repeated multiplication shrinks gradients exponentially.  \n",
    "- **Result:** Early time steps receive near-zero updates (**vanishing gradients**).  \n",
    "\n",
    "\n",
    "**2. Impact of Lookback Window Size**  \n",
    "| Lookback Window ( T ) | Gradient Behavior | Model Performance |  \n",
    "|--------------------------|-------------------|-------------------|  \n",
    "| **Short** (e.g., \\( T=5 \\))  | Gradients stable | Captures only short-term patterns |  \n",
    "| **Long** (e.g., \\( T=50 \\)) | Gradients vanish exponentially | Fails to learn long-term dependencies |  \n",
    "\n",
    "**Example:** If each Jacobian $ \\approx 0.9 $, after T=50, $ 0.9^{50} \\approx 0.005 $.  \n",
    "\n",
    "\n",
    "**3. Solutions**  \n",
    "1. **LSTM/GRU:** Gated architectures mitigate vanishing gradients.  \n",
    "2. **Skip Connections:** Residual paths (e.g., Transformers) bypass Jacobian chain.  \n",
    "3. **Gradient Clipping:** Prevents exploding gradients.  \n",
    "\n",
    "**Conclusion:** Longer lookback worsens vanishing gradients; use specialized architectures for long sequences.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
