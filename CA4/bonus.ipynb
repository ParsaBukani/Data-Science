{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2032b74f",
   "metadata": {},
   "source": [
    "**<span style=\"color: gray;\">In The Name of God</span>**\n",
    "\n",
    "---\n",
    "\n",
    "# **Bonus**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb8b6d7",
   "metadata": {},
   "source": [
    "# **<span style=\"color: #99004C;\">Task#1 Bonus</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84a76c",
   "metadata": {},
   "source": [
    "##  When Does an MLP Become Equivalent to Logistic Regression?\n",
    "\n",
    "An **MLP (Multi-Layer Perceptron)** becomes equivalent to **logistic regression** under the following condition:\n",
    "\n",
    "1. **The network has only one linear layer** (**no hidden layers** )\n",
    "2. **Activation function is either:**\n",
    "   - sigmoid\n",
    "   - softmax\n",
    "3. **No Dropout or BatchNorm or complex components**\n",
    "   - Dropout adds random noise to the network (makes the model stochastic) and Logistic regression is a deterministic model.\n",
    "   - BatchNorm introduces a non-linear, dynamic transformation to the data.Logistic regression just applies a fixed linear transformation, no learned normalization0.Also, BatchNorm depends on the other data points in the batch â€” logistic regression doesn't.\n",
    "\n",
    "4. **No Non-linearities (ReLU, Tanh, etc.)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa84563c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **<span style=\"color: #99004C;\">Task#2 Bonus</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b76db59",
   "metadata": {},
   "source": [
    "# **<span style=\"color: #99004C;\">Task#3 Bonus</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d4ebf",
   "metadata": {},
   "source": [
    "### **Explain mathematically the vanishing gradient problem in Recurrent Neural Networks (RNNs). Then, analyze how changing the lookback window size impacts the severity of this phenomenon**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d42cb1",
   "metadata": {},
   "source": [
    "**1. Mathematical Explanation**  \n",
    "The gradient in RNNs involves multiplying Jacobians over time:  \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = \\sum_{k=1}^{T} \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial h_T}}_{\\text{Loss gradient}} \\underbrace{\\prod_{t=k}^{T-1} \\frac{\\partial h_{t+1}}{\\partial h_t}}_{\\text{Jacobian chain}} \\frac{\\partial h_k}{\\partial W}\n",
    "$$  \n",
    "\n",
    "- **Problem:** If $ \\left\\| \\frac{\\partial h_{t+1}}{\\partial h_t} \\right\\| < 1 $ repeated multiplication shrinks gradients exponentially.  \n",
    "- **Result:** Early time steps receive near-zero updates (**vanishing gradients**).  \n",
    "\n",
    "\n",
    "**2. Impact of Lookback Window Size**  \n",
    "| Lookback Window ( T ) | Gradient Behavior | Model Performance |  \n",
    "|--------------------------|-------------------|-------------------|  \n",
    "| **Short** (e.g., \\( T=5 \\))  | Gradients stable | Captures only short-term patterns |  \n",
    "| **Long** (e.g., \\( T=50 \\)) | Gradients vanish exponentially | Fails to learn long-term dependencies |  \n",
    "\n",
    "**Example:** If each Jacobian $ \\approx 0.9 $, after T=50, $ 0.9^{50} \\approx 0.005 $.  \n",
    "\n",
    "\n",
    "**3. Solutions**  \n",
    "1. **LSTM/GRU:** Gated architectures mitigate vanishing gradients.  \n",
    "2. **Skip Connections:** Residual paths (e.g., Transformers) bypass Jacobian chain.  \n",
    "3. **Gradient Clipping:** Prevents exploding gradients.  \n",
    "\n",
    "**Conclusion:** Longer lookback worsens vanishing gradients; use specialized architectures for long sequences.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
